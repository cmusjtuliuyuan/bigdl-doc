# Overview


* [BigDL 0.1.0 Java Doc][javadoc]
* [Building](#building)
* [Getting Started](#getting-started)
* [Python Support](#python-support)
* [Tutorials](#tutorials)
* [Visualization with TensorBoard](#visualization-with-tensorboard)
* [Running on EC2](#running-on-ec2)
* [Examples](#examples)
* [Programming Guide](#programming-guide)
* [Known Issues](#known-issues)

[javadoc]: javadoc  

---
## Building
This page shows how to install and build BigDL (on both Linux and macOS), including:
  

* [Linking](#linking)
* [Download](#download)
* [Build](#build)    
    * [Build with make-dist.sh](#build-with-make-distsh)  
        * [Using the `make-dist.sh` script](#build-with-make-distsh)
        * [Build for macOS](#build-for-macos)
        * [Build for Spark 2.0](#build-for-spark-20)
        * [Build for Spark 2.1](#build-for-spark-21)
        * [Build for Cloudera Distributed Spark](#build-for-cloudera-distributed-spark)
        * [Build using Scala 2.10 or 2.11](#build-using-scala-210-or-211)
        * [Full Build](#full-build)
    * [Build with Maven](#build-with-maven)
* [IDE Settings](#ide-settings)
* [Next Steps](#next-steps)

### Linking

Currently, BigDL is host on maven central, here's an example to add the dependency to your own project:
```xml
<dependency>
    <groupId>com.intel.analytics.bigdl</groupId>
    <artifactId>bigdl</artifactId>
    <version>0.1.0</version>
</dependency>
```
SBT developers can use
```sbt
libraryDependencies += "com.intel.analytics.bigdl" % "bigdl" % "0.1.0"
```
**Note**: the BigDL lib default supports Spark 1.5.x and 1.6.x; if your project runs on Spark 2.0 and 2.1, use this
```xml
<dependency>
    <groupId>com.intel.analytics.bigdl</groupId>
    <artifactId>bigdl-SPARK_2.0</artifactId>
    <version>0.1.0</version>
</dependency>
```

If your project runs on MacOS, you should add the dependency below,

```
<dependency>
    <groupId>com.intel.analytics.bigdl.native</groupId>
    <artifactId>mkl-java-mac</artifactId>
    <version>0.1.0</version>
    <exclusions>
        <exclusion>
            <groupId>com.intel.analytics.bigdl.native</groupId>
            <artifactId>bigdl-native</artifactId>
        </exclusion>
    </exclusions>
</dependency>
```

SBT developers can use
```sbt
libraryDependencies += "com.intel.analytics.bigdl" % "bigdl-SPARK_2.0" % "0.1.0"
```
### Download
BigDL source code is available at [GitHub](https://github.com/intel-analytics/BigDL)

```sbt
$ git clone https://github.com/intel-analytics/BigDL.git
```

### Build
Maven 3 is needed to build BigDL, you can download it from the [maven website](https://maven.apache.org/download.cgi). 

After installing Maven 3, please set the environment variable MAVEN_OPTS as follows:
```sbt
$ export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
```
When compiling with Java 7, you need to add the option “-XX:MaxPermSize=1G”. 

#### Build with `make-dist.sh` 
It is highly recommended that you build BigDL using the [make-dist.sh](https://github.com/intel-analytics/BigDL/blob/master/make-dist.sh).

Once downloaded, you can build BigDL with the following commands:
```sbt
$ bash make-dist.sh
```
After that, you can find a `dist` folder, which contains all the needed files to run a BigDL program. The files in `dist` include:  

 * **dist/bin/bigdl.sh**: A script used to set up proper environment variables and launch the BigDL program.
 * **dist/lib/bigdl-VERSION-jar-with-dependencies.jar**: This jar package contains all dependencies except Spark.

##### Build for macOS
The instructions above will only build for Linux. To build BigDL for macOS, pass `-P mac` to the `make-dist.sh` script as follows:
```{r, engine='sh'}
$ bash make-dist.sh -P mac
```
##### Build for Spark 2.0
The instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10); to build for Spark 2.0 (which uses Scala 2.11 by default), pass `-P spark_2.0` to the `make-dist.sh` script:
```sbt
$ bash make-dist.sh -P spark_2.0
```
It is highly recommended to use _**Java 8**_ when running with Spark 2.0; otherwise you may observe very poor performance.

##### Build for Spark 2.1
The instructions above will build BigDL with Spark 1.5.x or 1.6.x or spark 2.0; to build for Spark 2.1 (which uses Scala 2.11 by default), pass `-P spark_2.1` to the `make-dist.sh` script:
```sbt
$ bash make-dist.sh -P spark_2.1
```

It is highly recommended to use _**Java 8**_ when running with Spark 2.x; otherwise you may observe very poor performance.

##### Build for Cloudera Distributed Spark
The instructions above will build BigDL with Spark2-2.0.0-cloudera2 and cdh5-1.6.0_* ; to build for Spark2-2.0.0-cloudera2 (which uses Scala 2.11 by default), pass `-P cloudera` to the `make-dist.sh` script:
```sbt
$ bash make-dist.sh -P cloudera
```
To build for cdh5-1.6.0_*(which uses Scala 2.10 by default) with the following commands:
```sbt
$ bash make-dist.sh
```
##### Build using Scala 2.10 or 2.11
By default, `make-dist.sh` uses Scala 2.10 for Spark 1.5.x or 1.6.x, and Scala 2.11 for Spark 2.0. To override the default behaviors, you can pass `-P scala_2.10` or `-P scala_2.11` to `make-dist.sh` as appropriate.

##### Full Build

Note that the instructions above will skip the build of native library code, and pull the corresponding libraries from Maven Central. If you want to build the the native library code by yourself, follow the steps below:

1. Download and install [Intel Parallel Studio XE](https://software.intel.com//qualify-for-free-software/opensourcecontributor) in your Linux box.

2. Prepare build environment as follows:
```sbt
    $ source <install-dir>/bin/compilervars.sh intel64
    $ source PATH_TO_MKL/bin/mklvars.sh intel64
```
where the `PATH_TO_MKL` is the installation directory of the MKL.  
3. Full build  
Clone BidDL as follows:  
```sbt
   git clone git@github.com:intel-analytics/BigDL.git --recursive 
```
   For already cloned repos, just use:  
```sbt
   git submodule update --init --recursive 
```
If the Intel MKL is not installed to the default path `/opt/intel`, please pass your libiomp5.so's directory path tothe `make-dist.sh` script:  
```sbt
   $ bash make-dist.sh -P full-build -DiompLibDir=<PATH_TO_LIBIOMP5_DIR> 
```
Otherwise, only pass `-P full-build` to the `make-dist.sh` script:
```sbt
   $ bash make-dist.sh -P full-build
```
#### Build with Maven
To build BigDL directly using Maven, run the command below:

```sbt
  $ mvn clean package -DskipTests
```
After that, you can find that the three jar packages in `PATH_To_BigDL`/target/, where `PATH_To_BigDL` is the path to the directory of the BigDL. 

Note that the instructions above will build BigDL with Spark 1.5.x or 1.6.x (using Scala 2.10) for Linux, and skip the build of native library code. Similarly, you may customize the default behaviors by passing the following parameters to maven:

 * `-P mac`: build for maxOS
 * `-P spark_2.0`: build for Spark 2.0 (using Scala 2.11). (Again, it is highly recommended to use _**Java 8**_ when running with Spark 2.0; otherwise you may observe very poor performance.)
 * `-P full-build`: full build
 * `-P scala_2.10` (or `-P scala_2.11`): build using Scala 2.10 (or Scala 2.11) 

#### IDE Settings
We set the scope of spark related library to `provided` in pom.xml. The reason is that we don't want package spark related jars which will make bigdl a huge jar, and generally as bigdl is invoked by spark-submit, these dependencies will be provided by spark at run-time.

This will cause a problem in IDE. When you run applications, it will throw `NoClassDefFoundError` because the library scope is `provided`.

You can easily change the scopes by the `all-in-one` profile.

* In Intellij, go to View -> Tools Windows -> Maven Projects. Then in the Maven Projects panel, Profiles -> click "all-in-one". 

#### Next Steps
* To learn how to run BigDL programs (as either a local Java program or a Spark program), you can check out the [Getting Started Page](#getting-started).
* To learn the details of Python support in BigDL, you can check out the [Python Support Page](#python-support)
---
##Getting Started

This page shows how to run a BigDL program, including  

* [Before running a BigDL program](#before-running-a-bigdl-program)
* [Interactive Spark Shell](#interactive-spark-shell)
* [Spark Program](#spark-program)
* [Next Steps](#next-steps)

### Before running a BigDL program
Before running a BigDL program, you need to set proper environment variables first.

#### Setting Environment Variables.
To achieve high performance, BigDL uses Intel MKL and multi-threaded programming; therefore, you need to first set the environment variables by running the provided script in `PATH_To_BigDL/bin/bigdl.sh` as follows:
```sbt
$ source PATH_To_BigDL/bin/bigdl.sh
```
Alternatively, you can also use the `PATH_To_BigDL/bin/bigdl.sh` script to launch your BigDL program; see the details below.

### Interactive Spark shell
You can quickly experiment with BigDL codes as a Spark program using the interactive Spark shell by running:
```sbt
$ source PATH_To_BigDL/bin/bigdl.sh
$ SPARK_HOME/bin/spark-shell --properties-file dist/conf/spark-bigdl.conf    \
  --jars bigdl-VERSION-jar-with-dependencies.jar
```
Then you can see something like:
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)
Spark context available as sc.
scala> 
```

For instance, to experiment with the ````Tensor```` APIs in BigDL, you may then try:
```scala
scala> import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Tensor

scala> Tensor[Double](2,2).fill(1.0)
res9: com.intel.analytics.bigdl.tensor.Tensor[Double] =
1.0     1.0
1.0     1.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
```
For more details about the BigDL APIs, please refer to the [Programming Guide](#Programming-Guide).

### Spark Program  

You can run a BigDL program, e.g., the [VGG](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/vgg) training, as a standard Spark program (running in either local mode or cluster mode) as follows:

1. Download the CIFAR-10 data from [here](https://www.cs.toronto.edu/%7Ekriz/cifar.html). Remember to choose the binary version.

2. Use the `bigdl.sh` script to launch the example as a Spark program as follows:

```bash
  # Spark local mode
  ./dist/bin/bigdl.sh -- \
  spark-submit --master local[core_number] --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size

  # Spark standalone mode
  ./dist/bin/bigdl.sh -- \
  spark-submit --master spark://... --executor-cores cores_per_executor \
  --total-executor-cores total_cores_for_the_job \
  --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size

  # Spark yarn mode
  ./dist/bin/bigdl.sh -- \
  spark-submit --master yarn --deploy-mode client \
  --executor-cores cores_per_executor \
  --num-executors executors_number \
  --class com.intel.analytics.bigdl.models.vgg.Train \
  dist/lib/bigdl-VERSION-jar-with-dependencies.jar \
  -f path_to_your_cifar_folder \
  -b batch_size
```

  The parameters used in the above command are:

  * -f: The folder where your put the CIFAR-10 data set. Note in this example, this is just a local file folder on the Spark driver; as the CIFAR-10 data is somewhat small (about 120MB), we will directly send it from the driver to executors in the example.

  * -b: The mini-batch size. The mini-batch size is expected to be a multiple of **total cores** used in the job. In this example, the mini-batch size is suggested to be set to **total cores * 4**

### Next Steps
* To learn the details of Python support in BigDL, you can check out the [Python Support Page](#Python-Support)
* To learn how to create practical neural networks using BigDL in a couple of minutes, you can check out the [Tutorials Page](#tutorials)
* You can check out the [Document Page](https://github.com/intel-analytics/BigDL/wiki/Documents) for more details (including Tutorials, Examples, Programming Guide, etc.)
* You can join the [BigDL Google Group](https://groups.google.com/forum/#!forum/bigdl-user-group) (or subscribe to the [mail list](mailto:bigdl-user-group+subscribe@googlegroups.com)) for more questions and discussions on BigDL
* You can post bug reports and feature requests at the [Issue Page](https://github.com/intel-analytics/BigDL/issues)
---  
##Python Support

* [BigDL Python API](#bigdl-python-api)
    * [Tutorial: Text Classification using BigDL Python API](#tutorial-text-classification-using-bigdl-python-api)
* [Running BigDL Python Program](#running-bigdl-python-programs)
    * [Automatically Packaging Python Dependency](#automatically-packaging-python-dependency)
* [Running BigDL Python Code in Notebooks](#running-bigdl-python-code-in-notebooks)

###BigDL Python API
Python is one of the most widely used language in the big data and data science community, and BigDL provides full support for Python APIs (built on top of PySpark), which are similar to the [Scala interface](https://www.javadoc.io/doc/com.intel.analytics.bigdl/bigdl/0.1.0-doc). (Please note currently the Python support has been tested with Python 2.7 and Spark 1.6 / Spark 2.0). 

With the full Python API support in BigDL, users can use deep learning models in BigDL together with existing Python libraries (e.g., Numpy and Pandas), which automatically runs in a distributed fashion to process large volumes of data on Spark.  

##### Tutorial: Text Classification using BigDL Python API  

This tutorial describes the [textclassifier](https://github.com/intel-analytics/BigDL/tree/master/pyspark/dl/models/textclassifier) example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by [this Keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html))

The example first creates the `SparkContext` using the SparkConf` return by the `create_spark_conf()` method, and then initialize the engine:
```python
  sc = SparkContext(appName="text_classifier",
                    conf=create_spark_conf())
  init_engine()
```

It then loads the [20 Newsgroup dataset](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) into RDD, and transforms the input data into an RDD of `Sample`. (Each `Sample` in essence contains a tuple of two NumPy ndarray representing the feature and label).

```python
  texts = news20.get_news20()
  data_rdd = sc.parallelize(texts, 2)
  ...
  sample_rdd = vector_rdd.map(
      lambda (vectors, label): to_sample(vectors, label, embedding_dim))
  train_rdd, val_rdd = sample_rdd.randomSplit(
      [training_split, 1-training_split])   
```

After that, the example creates the neural network model as follows:
```python
def build_model(class_num):
    model = Sequential()

    if model_type.lower() == "cnn":
        model.add(Reshape([embedding_dim, 1, sequence_len]))
        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))
        model.add(ReLU())
        model.add(SpatialMaxPooling(5, 1, 5, 1))
        model.add(SpatialConvolution(128, 128, 5, 1))
        model.add(ReLU())
        model.add(SpatialMaxPooling(5, 1, 5, 1))
        model.add(Reshape([128]))
    elif model_type.lower() == "lstm":
        model.add(Recurrent()
                  .add(LSTM(embedding_dim, 128)))
        model.add(Select(2, -1))
    elif model_type.lower() == "gru":
        model.add(Recurrent()
                  .add(GRU(embedding_dim, 128)))
        model.add(Select(2, -1))
    else:
        raise ValueError('model can only be cnn, lstm, or gru')

    model.add(Linear(128, 100))
    model.add(Linear(100, class_num))
    model.add(LogSoftMax())
    return model
```
Finally the example creates the `Optimizer` (which accepts both the model and the training Sample RDD) and trains the model by calling `Optimizer.optimize()`:

```python
optimizer = Optimizer(
    model=build_model(news20.CLASS_NUM),
    training_rdd=train_rdd,
    criterion=ClassNLLCriterion(),
    end_trigger=MaxEpoch(max_epoch),
    batch_size=batch_size,
    optim_method="Adagrad",
    state=state)
...
train_model = optimizer.optimize()
```

### Running BigDL Python Programs
A BigDL Python program runs as a standard PySPark program, which requires all Python dependency (e.g., NumPy) used by the program be installed on each node in the Spark cluster. One can run the BigDL [lenet Python example](https://github.com/intel-analytics/BigDL/tree/master/pyspark/dl/models/lenet) using [spark-submit](http://spark.apache.org/docs/latest/submitting-applications.html) as follows:
```bash
PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-python-api.zip
BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-VERSION-jar-with-dependencies.jar
PYTHONPATH=${PYTHON_API_ZIP_PATH}:$PYTHONPATH
   
source ${BigDL_HOME}/dist/bin/bigdl.sh
   
${SPARK_HOME}/bin/spark-submit \
    --master ${MASTER} \
    --driver-memory 10g  \
    --driver-cores 4  \
    --executor-memory 20g \
    --total-executor-cores ${TOTAL_CORES}\
    --executor-cores 10 ${EXECUTOR_CORES} \
    --py-files ${PYTHON_API_PATH},${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py  \
    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \
    --jars ${BigDL_JAR_PATH} \
    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \
    --conf spark.executor.extraClassPath=bigdl-VERSION-jar-with-dependencies.jar \
    ${BigDL_HOME}/pyspark/dl/models/lenet/lenet5.py
```
##### Automatically Packaging Python Dependency

You can run BigDL Python programs on YARN clusters without changes to the cluster (e.g., no need to pre-install the Python dependency). You  can first package all the required Python dependency into a virtual environment on the local node (where you will run the spark-submit command), and then directly use spark-submit to run the BigDL Python program on the YARN cluster (using that virtual environment). Please refer to this [patch](https://github.com/intel-analytics/BigDL/pull/706) for more details.

### Running BigDL Python Code in Notebooks
With the full Python API support in BigDL, users can now use BigDL together with powerful notebooks (such as Jupyter notebook) in a distributed fashion across the cluster, combining Python libraries, Spark SQL / dataframes and MLlib, deep learning models in BigDL, as well as interactive visualization tools.

First, install all the necessary libraries on the local node where you will run Jupyter, e.g., 
```bash
sudo apt install python
sudo apt install python-pip
sudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud
```

Then, you can launch the Jupyter notebook as follows:
```bash
PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-python-api.zip
BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-0.1.0-jar-with-dependencies.jar

export PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --notebook-dir=./ --ip=* --no-browser"

source ${BigDL_HOME}/dist/bin/bigdl.sh

${SPARK_HOME}/bin/pyspark \
  --master ${MASTER} \
  --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \
  --driver-memory 10g  \
  --driver-cores 4  \
  --executor-memory 20g \
  --total-executor-cores {TOTAL_CORES} \
  --executor-cores {EXECUTOR_CORES} \
  --py-files ${PYTHON_API_PATH} \
  --jars ${BigDL_JAR_PATH} \
  --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \
  --conf spark.executor.extraClassPath=bigdl-0.1.0-jar-with-dependencies.jar
```

After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using your browser. You can find the exact URL in the console output when you started Jupyter; by default, the dashboard URL is http://your_node:8888/

---
## Tutorials

This page shows how to build simple deep learning programs using BigDL, including:

1. [Training LeNet on MNIST](#training-lenet-on-mnist) - the "hello world" for deep learning
2. [Text Classification](#text-classification-working-with-spark-rdd) - working with Spark RDD transformations
3. [Image Classification](#image-classification-working-with-spark-dataframe-and-ml-pipeline) - working with Spark DataFrame and ML pipeline
4. [Python Text Classifier](#tutorial-text-classification-using-bigdl-python-api) text classification using BigDL Python APIs
5. [Jupyter Notebook Tutorial](https://github.com/intel-analytics/BigDL/blob/branch-0.1/pyspark/dl/example/tutorial/simple_text_classification/text_classfication.ipynb) - using BigDL Python APIs in Jupyter notebook

### Training LeNet on MNIST
This tutorial is an explanation of what is happening in the [lenet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet/Train.scala) example, which trains [LeNet-5](http://yann.lecun.com/exdb/lenet/) on the [MNIST data](http://yann.lecun.com/exdb/mnist/) using BigDL.

A BigDL program starts with `import com.intel.analytics.bigdl._`; it then _**creates the `SparkContext`**_ using the `SparkConf` returned by the `Engine`; after that, it _**initializes the `Engine`**_.
````scala
  val conf = Engine.createSparkConf()
      .setAppName("Train Lenet on MNIST")
      .set("spark.task.maxFailures", "1")
  val sc = new SparkContext(conf)
  Engine.init
````
````Engine.createSparkConf```` will return a ````SparkConf```` populated with some appropriate configuration. And ````Engine.init```` will verify and read some environment information(e.g. executor numbers and executor cores) from the ````SparkContext````. You can find more information about the initialization in the [Programming Guide](#engine)

After the initialization, we need to:

1. _**Create the LeNet model**_ by calling the [````LeNet5()````](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet/LeNet5.scala), which creates the LeNet-5 convolutional network model as follows:

```scala
    val model = Sequential()
    model.add(Reshape(Array(1, 28, 28)))
      .add(SpatialConvolution(1, 6, 5, 5))
      .add(Tanh())
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Tanh())
      .add(SpatialConvolution(6, 12, 5, 5))
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Reshape(Array(12 * 4 * 4)))
      .add(Linear(12 * 4 * 4, 100))
      .add(Tanh())
      .add(Linear(100, classNum))
      .add(LogSoftMax())
```

2. Load the data by _**creating the [```DataSet```](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/dataset)**_ (either a distributed or local one depending on whether it runs on Spark or not), and then _**applying a series of [```Transformer```](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/dataset)**_ (e.g., ````SampleToGreyImg````, ````GreyImgNormalizer```` and ````GreyImgToBatch````):

```scala
    val trainSet = (if (sc.isDefined) {
        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)
      } else {
        DataSet.array(load(trainData, trainLabel))
      }) -> SampleToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(
        param.batchSize)
```

After that, we _**create the [```Optimizer```](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/optim)**_ (either a distributed or local one depending on whether it runs on Spark or not) by specifying the ````DataSet````, the model and the ````Criterion```` (which, given input and target, computes gradient per given loss function):
````scala
  val optimizer = Optimizer(
    model = model,
    dataset = trainSet,
    criterion = ClassNLLCriterion[Float]())
````

Finally (after optionally specifying the validation data and methods for the ````Optimizer````), we _**train the model by calling ````Optimizer.optimize()````**_:
````scala
  optimizer
    .setValidation(
      trigger = Trigger.everyEpoch,
      dataset = validationSet,
      vMethods = Array(new Top1Accuracy))
    .setState(state)
    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))
    .optimize()
````

### Text Classification - Working with Spark RDD
This tutorial describes the [text_classification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification) example, which builds a text classifier using a simple convolutional neural network (CNN) model. (It was first described by [this Keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)).

After importing ```com.intel.analytics.bigdl._``` and some initialization, the [example](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification/TextClassifier.scala) broadcasts the pre-trained world embedding and loads the input data using RDD transformations:
````scala
  // For large dataset, you might want to get such RDD[(String, Float)] from HDFS
  val dataRdd = sc.parallelize(loadRawData(), param.partitionNum)
  val (word2Meta, word2Vec) = analyzeTexts(dataRdd)
  val word2MetaBC = sc.broadcast(word2Meta)
  val word2VecBC = sc.broadcast(word2Vec)
  val vectorizedRdd = dataRdd
      .map {case (text, label) => (toTokens(text, word2MetaBC.value), label)}
      .map {case (tokens, label) => (shaping(tokens, sequenceLen), label)}
      .map {case (tokens, label) => (vectorization(
        tokens, embeddingDim, word2VecBC.value), label)}
````

The [example](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification/TextClassifier.scala) then converts the processed data (`vectorizedRdd`) to an RDD of Sample, and randomly splits the sample RDD (`sampleRDD`) into training data (`trainingRDD`) and validation data (`valRDD`):
````scala
  val sampleRDD = vectorizedRdd.map {case (input: Array[Array[Float]], label: Float) =>
        Sample(
          featureTensor = Tensor(input.flatten, Array(sequenceLen, embeddingDim))
            .transpose(1, 2).contiguous(),
          labelTensor = Tensor(Array(label), Array(1)))
      }

  val Array(trainingRDD, valRDD) = sampleRDD.randomSplit(
    Array(trainingSplit, 1 - trainingSplit))
````

After that, the [example](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification/TextClassifier.scala) builds the CNN model, creates the ````Optimizer````, pass the RDD of training data (`trainingRDD`) to the ```Optimizer``` (with specific batch size), and finally trains the model (using ````Adagrad```` as the optimization method, and setting relevant hyper parameters in ````state````):
````scala
  val optimizer = Optimizer(
    model = buildModel(classNum),
    sampleRDD = trainingRDD,
    criterion = new ClassNLLCriterion[Float](),
    batchSize = param.batchSize
  )
  val state = T("learningRate" -> 0.01, "learningRateDecay" -> 0.0002)
  optimizer
    .setState(state)
    .setOptimMethod(new Adagrad())
    .setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)
    .setEndWhen(Trigger.maxEpoch(2))
    .optimize()
````

### Image Classification - Working with Spark DataFrame and ML pipeline
This tutorial describes the [image_classification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/imageclassification) example, which loads a BigDL ([Inception](http://arxiv.org/abs/1409.4842)) model or Torch ([Resnet](https://arxiv.org/abs/1512.03385)) model that is trained on [ImageNet](http://image-net.org/download-images) data, and then applies the loaded model to predict the contents of a set of images using BigDL and Spark [ML pipeline](https://spark.apache.org/docs/1.6.3/ml-guide.html).

After importing ```com.intel.analytics.bigdl._``` and some initialization, the [example](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/imageclassification/ImagePredictor.scala) first [loads](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/imageclassification/MlUtils.scala) the specified model:

```scala
  def loadModel[@specialized(Float, Double) T : ClassTag](param : PredictParams)
    (implicit ev: TensorNumeric[T]): Module[T] = {
    val model = param.modelType match {
      case TorchModel =>
        Module.loadTorch[T](param.modelPath)
      case BigDlModel =>
        Module.load[T](param.modelPath)
      case _ => throw new IllegalArgumentException(s"${param.modelType}")
    }
    model
  }
```

It then creates ```DLClassifer``` (a Spark ML pipelines [Transformer](https://spark.apache.org/docs/1.6.3/ml-pipeline.html#transformers)) that predicts the input value based on the specified deep learning model:
```scala
  val model = loadModel(param)
  val valTrans = new DLClassifier()
    .setInputCol("features")
    .setOutputCol("predict")

  val paramsTrans = ParamMap(
    valTrans.modelTrain -> model,
    valTrans.batchShape ->
    Array(param.batchSize, 3, imageSize, imageSize))
```

After that, the [example](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/imageclassification/ImagePredictor.scala)  loads the input images into a [DataFrame](https://spark.apache.org/docs/1.6.3/ml-guide.html#dataframe), and then predicts the class of each each image using the ```DLClassifer```:
```scala
  val valRDD = sc.parallelize(imageSet).repartition(partitionNum)
  val transf = RowToByteRecords() ->
      SampleToBGRImg() ->
      BGRImgCropper(imageSize, imageSize) ->
      BGRImgNormalizer(testMean, testStd) ->
      BGRImgToImageVector()

  val valDF = transformDF(sqlContext.createDataFrame(valRDD), transf)

  valTrans.transform(valDF, paramsTrans)
      .select("imageName", "predict")
      .show(param.showNum)
```
---
## Visualization with TensorBoard

### Generating summary info in BigDL
To enable visualization support, you need first properly configure the `Optimizer` to generate summary info for training (`TrainSummary`) and/or validation (`ValidationSummary`) before invoking `Optimizer.optimize()`, as illustrated below: 

_**Generating summary info in Scala**_
```scala
val optimizer = Optimizer(...)
...
val logdir = "mylogdir"
val appName = "myapp"
val trainSummary = TrainSummary(logdir, appName)
val talidationSummary = ValidationSummary(logdir, appName)
optimizer.setTrainSummary(trainSummary)
optimizer.setValidationSummary(validationSummary)
...
val trained_model = optimizer.optimize()
```
_**Generating summary info in Python**_
```python
optimizer = Optimizer(...)
...
log_dir = 'mylogdir'
app_name = 'myapp'
train_summary = TrainSummary(log_dir=log_dir, app_name=app_name)
val_summary = ValidationSummary(log_dir=log_dir, app_name=app_name)
optimizer.set_train_summary(train_summary)
optimizer.set_val_summary(val_summary)
...
trainedModel = optimizer.optimize()
```
After you start to run your spark job, the train and validation summary will be saved to `mylogdir/myapp/train` and `mylogdir/myapp/validation` respectively (Note: you may want to use different `appName` for different job runs to avoid possible conflicts.) You may then read the summary info as follows:

_**Reading summary info in Scala**_
```scala
val trainLoss = trainSummary.readScalar("Loss")
val validationLoss = validationSummary.readScalar("Loss")
...
```

_**Reading summary info in Python**_
```python
loss = np.array(train_summary.read_scalar('Loss'))
valloss = np.array(val_summary.read_scalar('Loss'))
...
```

### Visualizing training with TensorBoard
With the summary info generated, we can then use [TensorBoard](https://pypi.python.org/pypi/tensorboard) to visualize the behaviors of the BigDL program.  

#### Installing TensorBoard
Prerequisites:
* Python verison: 2.7, 3.4, 3.5, or 3.6
* Pip version >= 9.0.1

To install TensorBoard using Python 2, you may run the command:
```bash
pip install tensorboard==1.0.0a4
```

To install TensorBoard using Python 3, you may run the command:
```bash
pip3 install tensorboard==1.0.0a4
```

Please refer to [this page](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/visualization#known-issues) for possible issues when installing TensorBoard.

#### Launching TensorBoard
You can launch TensorBoard using the command below:
```
tensorboard --logdir=/tmp/bigdl_summaries
```
After that, navigate to the TensorBoard dashboard using a browser. You can find the URL in the console output after TensorBoard is successfully launched; by default the URL is http://your_node:6006

#### Visualizations in TensorBoard
Within the TensorBoard dashboard, you will be able to read the visualizations of each run, including the “Loss” and “Throughput” curves under the SCALARS tab (as illustrated below):
![Scalar](tensorboard_scalar.png)

And “weights”, “bias”, “gradientWeights” and “gradientBias” under the DISTRIBUTIONS and HISTOGRAMS tabs (as illustrated below):
![histogram1](tensorboard_histo1.png)

![histogram2](tensorboard_histo2.png)

---
## Running on EC2

* [1. AMI](#1-ami)
* [2. Before You Start](#2-before-you-start)
* [3. Run BigDL examples](#3-run-bigdl-examples)
    * [3.1 Run the "inception-v1" example](#31-run-the-inception-v1-example)
    * [3.2 Run the "perf" example](#32-run-the-perf-example)

### 1. AMI
To make it easier to try out BigDL examples on Spark using EC2, a public AMI is provided. It will automatically retrieve the latest BigDL package, download the necessary input data, and then run the specified BigDL example (using Java 8 on a Spark cluster). The details of the public AMI are shown in the table below.

|BigDL version |AMI version|Date        |AMI ID      |AMI Name          |Region               |Status    |
|--------------|-----------|------------|------------|----------------- |---------------------|----------|
|master        |0.2S       |Mar 13, 2017|ami-37b73957|BigDL Client 0.2S |US West (Oregon)     |Active    |
|master        |0.2S       |Apr 10, 2017|ami-8c87099a|BigDL Client 0.2S |US East (N. Virginia)|Active    |
|0.1.0         |0.1.0      |Apr 10, 2017|ami-9a8818fa|BigDL Client 0.1.0|US West (Oregon)     |Active    |
|0.1.0         |0.1.0      |Apr 10, 2017|ami-6476f872|BigDL Client 0.1.0|US East (N. Virginia)|Active    |


Please note that it is highly recommended to run BigDL using EC2 instances with Xeon E5 v3 or v4 processors.

After launching the AMI on EC2, please log on to the instance and run a "bootstrap.sh" script to download example scripts.

```bash
./bootstrap.sh
```

### 2. Before You Start
Before running the BigDL examples, you need to launch a Spark cluster on EC2 (you may refer to [https://github.com/amplab/spark-ec2](https://github.com/amplab/spark-ec2) for more instructions). In addition, to run the Inception-v1 example, you also need to start a HDFS cluster on EC2 to store the input image data.

### 3. Run BigDL examples
You can run BigDL examples using the `run.example.sh` script in home directory of your BigDL Client instance (e.g. `/home/ubuntu/`) with the following parameters:
* Mandatory parameters:
  * `-m|--model` which model to train, including
    * lenet: train the [LeNet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet) example
    * vgg: train the [VGG](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/vgg) example
    * inception-v1: train the [Inception v1](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception) example
    * perf: test the training speed using the [Inception v1](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception/Inception_v1.scala) model with dummy data

  * `-s|--spark-url` the master URL for the Spark cluster

  * `-n|--nodes` number of Spark slave nodes

  * `-o|--cores` number of cores used on each node

  * `-r|--memory` memory used on each node, e.g. 200g

  * `-b|--batch-size` batch size when training the model; it is expected to be a multiple of "nodes * cores"

  * `-f|--hdfs-data-dir` HDFS directory for the input images (for the "inception-v1" model training only)

* Optional parameters:
  * `-e|--max-epoch` the maximum number of epochs (i.e., going through all the input data once) used in the training; default to 90 if not specified

  * `-p|--spark` by default the example will run with Spark 1.5 or 1.6; to use Spark 2.0, please specify "spark_2.0" here (it is highly recommended to use _**Java 8**_ when running BigDL for Spark 2.0, otherwise you may observe very poor performance)

  * `-l|--learning-rate` by default the the example will use an initial learning rate of "0.01"; you can specify a different value here

After the training, you can check the log files and generated models in the home directory (e.g., `/home/ubuntu/`).  
  
#### 3.1 Run the "inception-v1" example  

You can refer to the [Inception v1](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception) example to prepare the input [ImageNet](http://image-net.org/index) data here. Alternatively, you may also download just a small set of images (with dummy labels) to run the example as follows, which can be useful if you only want to try it out to see the training speed on a Spark cluster.

* Download and prepare the input image data (a subset of the [Flickr Style](http://sergeykarayev.com/files/1311.3715v3.pdf) data)

```bash
  ./download.sh $HDFS-NAMENODE
```

  After the download completes, the downloaded images are stored in `hdfs://HDFS-NAMENODE:9000/seq`. (If the download fails with error "Unable to establish SSL connection." please check your network connection and retry this later.)

* To run the "inception-v1" example on a 4-worker Spark cluster (using, say, the "m4.10xlarge" instance), run the example command below: 

```
  nohup bash ./run.example.sh --model inception-v1  \
         --spark-url spark://SPARK-MASTER:7077    \
         --nodes 4 --cores 20 --memory 150g       \
         --batch-size 400 --learning-rate 0.0898  \
         --hdfs-data-dir hdfs://HDFS-NAMENODE:9000/seq \
         --spark spark_2.0 --max-epoch 4 \
         > incep.log 2>&1 &     
```

* View output of the training in the log file generated by the previous step:
```bash
  $ tail -f incep.log
  2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
  2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
```  
#### 3.2 Run the "perf" example

To run the "perf" example on a 4-worker Spark cluster (using, say, the "m4.10xlarge" instance), you may try the example command below: 

```bash
  nohup bash ./run.example.sh --model perf  \
       --spark-url spark://SPARK-MASTER:7077    \
       --nodes 4 --cores 20 --memory 150g       \
       --spark spark_2.0 --max-epoch 4 \
       > perf.log 2>&1 &
```
---
## Examples 

BigDL provides many popular [neural network models](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models) and [deep learning examples](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example) for Apache Spark, including: 
### Models
  * [LeNet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet): it demonstrates how to use BigDL to train and evaluate the [LeNet-5](http://yann.lecun.com/exdb/lenet/) network on MNIST data.
  * [Inception](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/inception): it demonstrates how to use BigDL to train and evaluate [Inception v1](https://arxiv.org/abs/1409.4842) and [Inception v2](https://arxiv.org/abs/1502.03167) architecture on the ImageNet data.
  * [VGG](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/vgg): it demonstrates how to use BigDL to train and evaluate a [VGG-like](http://torch.ch/blog/2015/07/30/cifar.html) network on CIFAR-10 data.
  * [ResNet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/resnet): it demonstrates how to use BigDL to train and evaluate the [ResNet](https://arxiv.org/abs/1512.03385) architecture on CIFAR-10 data.
  * [RNN](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn): it demonstrates how to use BigDL to build and train a simple recurrent neural network [(RNN) for language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf).
  * [Auto-encoder](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/autoencoder): it demonstrates how to use BigDL to build and train a basic fully-connected autoencoder using MNIST data.

### Examples
  * [text_classification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification): it demonstrates how to use BigDL to build a [text classifier](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) using a simple convolutional neural network (CNN) model.
  * [image_classification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/imageclassification): it demonstrates how to load a BigDL or [Torch](http://torch.ch/) model trained on ImageNet data (e.g., [Inception](https://arxiv.org/abs/1409.4842) or [ResNet](https://arxiv.org/abs/1512.03385)), and then applies the loaded model to classify the contents of a set of images in Spark ML pipeline.
  * [load_model](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/loadmodel): it demonstrates how to use BigDL to load a pre-trained [Torch](http://torch.ch/) or [Caffe](http://caffe.berkeleyvision.org/) model into Spark program for prediction.

### Python Examples
  * [LeNet](https://github.com/intel-analytics/BigDL/tree/master/pyspark/dl/models/lenet): it demonstrates how to use BigDL Python APIs to train and evaluate the [LeNet-5](http://yann.lecun.com/exdb/lenet/) network on MNIST data.
  * [Text Classifier](https://github.com/intel-analytics/BigDL/tree/master/pyspark/dl/models/textclassifier):  it demonstrates how to use BigDL Python APIs to build a text classifier using a simple [convolutional neural network (CNN) model(https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)] or a simple LSTM/GRU model.
  * [Jupyter tutorial](https://github.com/intel-analytics/BigDL/blob/branch-0.1/pyspark/dl/example/tutorial/simple_text_classification/text_classfication.ipynb): it contains a tutorial for using BigDL Python APIs in Jupyter notebooks (together with TensorBoard support) for interactive data explorations and visualizations.  
---
## Programming Guide

### Overview
Before starting the programming guide, you may have checked out the [Getting Started Page](#getting-started) and the [Tutorials page](#tutorials). This section will introduce the BigDL concepts and APIs for building deep learning applications on Spark.  

* [Tensor](#tensor)  
* [Table](#table)  
  * [Module](#module)  
    * [Create modules](#create-modules)  
    * [Construct complex networks](#construct-complex-networks)  
    * [Build neural network models](#build-neural-network-models)  
* [Criterion](#criterion)  
* [Transformer](#transformer)  
* [Sample and MiniBatch](#sample-and-minibatch)  
* [Engine](#engine)  
* [Optimizer](#optimizer)  
* [Validator](#validator)  
* [Model Persist](#model-persist)  
* [Logging](#logging)  
* [Visualization via TensorBoard](#visualization-via-tensorboard)  

### Tensor
Modeled after the [Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md) class in [Torch](http://torch.ch/), the ```Tensor``` [package](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor) (written in Scala and leveraging [Intel MKL](https://software.intel.com/en-us/intel-mkl)) in BigDL provides numeric computing support for the deep learning applications (e.g., the input, output, weight, bias and gradient of the neural networks).

A ```Tensor``` is essentially a multi-dimensional array of numeric types (e.g., ```Int```, ```Float```, ```Double```, etc.); you may check it out in the interactive Scala shell (by typing ```scala -cp bigdl_0.1-0.1.0-SNAPSHOT-jar-with-dependencies.jar```), for instance:
```scala
scala> import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.tensor.Tensor

scala> val tensor = Tensor[Float](2, 3)
tensor: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.0     0.0     0.0
0.0     0.0     0.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
```

### Table
Modeled after the [Table](https://github.com/torch/nn/blob/master/doc/table.md) class in [Torch](http://torch.ch/), the ```Table``` class (defined in package ```com.intel.analytics.bigdl.utils```) is widely used in BigDL (e.g., a ```Table``` of ```Tensor``` can be used as the input or output of neural networks). In essence, a ```Table``` can be considered as a key-value map, and there is also a syntax sugar to create a ```Table``` using ```T()``` in BigDL.

```scala
scala> import com.intel.analytics.bigdl.utils.T
import com.intel.analytics.bigdl.utils.T

scala> T(Tensor[Float](2,2), Tensor[Float](2,2))
res2: com.intel.analytics.bigdl.utils.Table =
 {
        2: 0.0  0.0
           0.0  0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
        1: 0.0  0.0
           0.0  0.0
           [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]
 }

```

### Module
Modeled after the [nn](https://github.com/torch/nn) package in [Torch](http://torch.ch/), the ```Module``` class in BigDL represents individual layers of the neural network (such as ```ReLU```, ```Linear```, ```SpatialConvolution```, ```Sequential```, etc.).

#### Create modules
For instance, we can create a ```Linear``` module as follows:
```scala
scala> import com.intel.analytics.bigdl.numeric.NumericFloat // import global float tensor numeric type
import com.intel.analytics.bigdl.numeric.NumericFloat

scala> import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.nn._

scala> val f = Linear(3,4) // create the module
mlp: com.intel.analytics.bigdl.nn.Linear[Float] = nn.Linear(3 -> 4)

// let's see what f's parameters were initialized to. ('nn' always inits to something reasonable)
scala> f.weight
res5: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.008662592    0.543819        -0.028795477
-0.30469555     -0.3909278      -0.10871882
0.114964925     0.1411745       0.35646403
-0.16590376     -0.19962183     -0.18782845
[com.intel.analytics.bigdl.tensor.DenseTensor of size 4x3]
```

#### Construct complex networks
We can use the ```Container``` module (e.g., ```Sequential```, ```Concat```, ```ConcatTable```, etc.) to combine individual models to build complex networks, for instance
```scala
scala> val g = Sum()
g: com.intel.analytics.bigdl.nn.Sum[Float] = nn.Sum

scala> val mlp = Sequential().add(f).add(g)
mlp: com.intel.analytics.bigdl.nn.Sequential[Float] =
nn.Sequential {
  [input -> (1) -> (2) -> output]
  (1): nn.Linear(3 -> 4)
  (2): nn.Sum
}
```

#### Build neural network models
We can create neural network models, e.g., [LeNet-5](http://yann.lecun.com/exdb/lenet/), using different ```Module``` as follows:

```scala
import com.intel.analytics.bigdl._
import com.intel.analytics.bigdl.numeric.NumericFloat
import com.intel.analytics.bigdl.nn._

object LeNet5 {
  def apply(classNum: Int): Module[Float] = {
    val model = Sequential()
    model.add(Reshape(Array(1, 28, 28)))
      .add(SpatialConvolution(1, 6, 5, 5))
      .add(Tanh())
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Tanh())
      .add(SpatialConvolution(6, 12, 5, 5))
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Reshape(Array(12 * 4 * 4)))
      .add(Linear(12 * 4 * 4, 100))
      .add(Tanh())
      .add(Linear(100, classNum))
      .add(LogSoftMax())
  }
}
```

### Criterion
Modeled after the [Criterion](https://github.com/torch/nn/blob/master/doc/criterion.md) class in [Torch](http://torch.ch/), the ```Criterion``` class in BigDL will compute loss and gradient (given prediction and target). See [BigDL Criterions](#Criterion) for a list of supported criterions. 
```scala
scala> val mse = MSECriterion() // mean square error lost, usually used for regression loss
mse: com.intel.analytics.bigdl.nn.MSECriterion[Float] = com.intel.analytics.bigdl.nn.MSECriterion@0

scala> val target = Tensor(3).rand() // create a target tensor randomly
target: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.33631626
0.2535103
0.94784033
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]

scala> val prediction = Tensor(3).rand() // create a predicted tensor randomly
prediction: com.intel.analytics.bigdl.tensor.Tensor[Float] =
0.91918194
0.6019384
0.38315287
[com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 3]

scala> mse.forward(prediction, target) // use mse to get the loss, returns 1/n sum_i (yhat_i - t_i)^2
res11: Float = 0.2600022

```
### Transformer
Transformer is for pre-processing. In many deep learning workload, input data need to be pre-processed before fed into model. For example, in CNN, the image file need to be decoded from some compressed format(e.g. jpeg) to float arrays, normalized and cropped to some fixed shape. You can also find pre-processing in other types of deep learning work load(e.g. NLP, speech recognition). In BigDL, we provide many pre-process procedures for user. They're implemented as Transformer.

The transformer interface is
```scala
trait Transformer[A, B] extends Serializable {
  def apply(prev: Iterator[A]): Iterator[B]
}
```

It's simple, right? What a transformer do is convert a sequence of objects of Class A to a sequence of objects of Class B.

Transformer is flexible. You can chain them together to do pre-processing. Let's still use the CNN example, say first we need read image files from given paths, then extract the image binaries to array of float, then normalized the image content and crop a fixed size from the image at a random position. Here we need 4 transformers, `PathToImage`, `ImageToArray`, `Normalizor` and `Cropper`. And then chain them together.
```scala
class PathToImage extends Transformer[Path, Image]
class ImageToArray extends Transformer[Image, Array]
class Normalizor extends Transformer[Array, Array]
class Cropper extends Transformer[Array, Array]

PathToImage -> ImageToArray -> Normalizor -> Cropper
```

Another benefit from `Transformer` is code reuse. You may find that for similar tasks, although there's a little difference, many pre-processing steps are same. So instead of a big single pre-process function, break it into small steps can improve the code reuse and save your time.

Transformer can work with Spark easily. For example, to transform RDD[A] to RDD[B]
```scala
val rddA : RDD[A] = ...
val tran : Transformer[A, B] = ...
val rddB : RDD[B] = rdd.mapPartitions(tran(_))
```

Transformer here is different from [Spark ML pipeline Transformer](https://spark.apache.org/docs/latest/ml-pipeline.html). But they serve similar purpose. 

### Sample and MiniBatch
**Sample** represent one `item` of your data set. For example, one image in image classification, one word in word2vec and one sentence in RNN language model.

**MiniBatch** represent `a batch of samples`. For computing efficiency, we would like to train/inference data in batches.

You need to convert your data type to Sample or MiniBatch by transformers, and then do optimization or inference. Please note that if you provide Sample format, BigDL will still convert it to MiniBatch automatically before optimization or inference.

### Engine
BigDL need some environment variables be set correctly to get a good performance. `Engine.init` method can help you set and verify them.

**How to do in the code?**
```scala
// Scala code example
val conf = Engine.createSparkConf()
val sc = new SparkContext(conf)
Engine.init
```
```python
# Python code example
conf=create_spark_conf()
sc = SparkContext(conf)
init_engine()
```
* If you're in spark-shell, Jupyter notebook or yarn-cluster

As the spark context is pre-created, you need start spark-shell or pyspark with `dist/conf/spark-bigdl.conf` file
```bash
# Spark shell
spark-shell --properties-file dist/conf/spark-bigdl.conf ...
# Jupyter notebook
pyspark --properties-file dist/conf/spark-bigdl.conf ...
```
In your code
```scala
Engine.init    // scala: check spark conf values
```
```python
init_engine()    # python: check spark conf values
```
### Optimizer
**Optimizer** represent a optimization process, aka training. 

You need to provide the model, train data set and loss function to start a optimization.
```scala
val optimizer = Optimizer(
  model = model,
  dataset = trainDataSet,
  criterion = new ClassNLLCriterion[Float]()
)
```

You can set other properties of a optimization process. Here's some examples:
* Hyper Parameter
```scala
optimizer.setState(
  T(
    "learningRate" -> 0.01,
    "weightDecay" -> 0.0005,
    "momentum" -> 0.9,
    "dampening" -> 0.0,
    "learningRateSchedule" -> SGD.EpochStep(25, 0.5)
  )
)
```
* Optimization method, the default one is SGD. See [Optimization Algorithms](https://github.com/intel-analytics/BigDL/wiki/Optimization-Algorithms) for a list of supported optimization methods and their usage.
```scala
// Change optimization method to adagrad
optimizer.setOptimMethod(new Adagrad())
```
* When to stop, the default one is stopped after 100 iteration
```scala
// Stop after 10 epoch
optimizer.setEndWhen(Trigger.maxEpoch(10))
```
* Checkpoint
```scala
// Every 50 iteration save current model and training status to ./checkpoint
optimizer.setCheckpoint("./checkpoint", Trigger.severalIteration(50))
```
* Validation
You can provide a separated data set for validation.
```scala
// Every epoch do a validation on valData, use Top1 accuracy metrics
optimizer.setValidation(Trigger.everyEpoch, valData, Array(new Top1Accuracy[Float]))
```

#### How BigDL train models in a distributed cluster?
BigDL distributed training is data parallelism. The training data is split among workers and cached in memory. A complete model is also cached on each worker. The model only uses the data of the same worker in the training.

BigDL employs a synchronous distributed training. In each iteration, each worker will sync the latest weights, calculate
gradients with local data and local model, sync the gradients and update the weights with a given optimization method(e.g. SGD, Adagrad).

In gradients and weights sync, BigDL doesn't use the RDD APIs like(broadcast, reduce, aggregate, treeAggregate). The problem of these methods is every worker needs to communicate with driver, so the driver will become the bottleneck if the parameter is too large or the workers are too many. Instead, BigDL implement a P2P algorithm for parameter sync to remove the bottleneck. For detail of the algorithm, please see the [code](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/optim/DistriOptimizer.scala)

### Validator
Validator represent testing the model with some metrics. The model can be loaded from disk or trained from optimization. The metrics can be Top1 accuracy, Loss, etc. See [Validation Methods](https://github.com/intel-analytics/BigDL/wiki/Validation-Methods) for a list of supported validation methods
```scala
// Test the model with validationSet and Top1 accuracy
val validator = Validator(model, validationSet)
val result = validator.test(Array(new Top1Accuracy[Float]))
```

### Model Persist
You can save your model like this
```scala
// Save as Java object
model.save("./model")

// Save as Torch object
model.saveTorch("./model.t7")
```
You can read your model file like this
```scala
// Load from Java object file
Module.load("./model")

// Load from torch file
Module.loadTorch("./model.t7")
```

### Logging
In the training, BigDL provide a straight forward logging like this. You can see epoch/iteration/loss/throughput directly from the log.
```
2017-01-10 10:03:55 INFO  DistriOptimizer$:241 - [Epoch 1 0/5000][Iteration 1][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
2017-01-10 10:03:58 INFO  DistriOptimizer$:241 - [Epoch 1 512/5000][Iteration 2][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
2017-01-10 10:04:00 INFO  DistriOptimizer$:241 - [Epoch 1 1024/5000][Iteration 3][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
2017-01-10 10:04:03 INFO  DistriOptimizer$:241 - [Epoch 1 1536/5000][Iteration 4][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
2017-01-10 10:04:05 INFO  DistriOptimizer$:241 - [Epoch 1 2048/5000][Iteration 5][Wall Clock XXX] Train 512 in XXXseconds. Throughput is XXX records/second. Loss is XXX.
```
The DistriOptimizer log level is INFO. Currently, we implement a method named with `redirectSparkInfoLogs` in `spark/utils/LoggerFilter.scala`. You can import and redirect at first.

```scala
import com.intel.analytics.bigdl.utils.LoggerFilter
LoggerFilter.redirectSparkInfoLogs()
```

This method will redirect all logs of `org`, `akka`, `breeze` to `bigdl.log` with `INFO` level, except `org.apache.spark.SparkContext`. And it will output all `ERROR` message in console too.

+ You can disable the redirection with java property `-Dbigdl.utils.LoggerFilter.disable=true`. By default, it will do redirect of all examples and models in our code.
+ You can set where the `bigdl.log` will be generated with `-Dbigdl.utils.LoggerFilter.logFile=<path>`. By default, it will be generated under current workspace.

### Visualization via TensorBoard
To enable visualization, you need to [install tensorboard](https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/visualization/README.md) first, then `setTrainSummary()` and `setValidationSummary()` to your optimizer before you call `optimize()`:
```scala
val logdir = "mylogdir"
val appName = "myapp"
val trainSummary = TrainSummary(logdir, appName)
val talidationSummary = ValidationSummary(logdir, appName)
optimizer.setTrainSummary(trainSummary)
optimizer.setValidationSummary(validationSummary)
```
After you start to run your spark job, the train and validation log will be saved to "mylogdir/myapp/train" and "mylogdir/myapp/validation". Notice: please change the appName before you start a new job, or the log files will conflict.

As the training started, use command `tensorboard --logdir mylogdir` to start tensorboard. Then open http://[ip]:6006 to watch the training.

* TrainSummary will show "Loss" and "Throughput" each iteration by default. You can use `setSummaryTrigger()` to enable "LearningRate" and "Parameters", or change the "Loss" and "Throughput"'s trigger:
```scala
trainSummary.setSummaryTrigger("LearningRate", Trigger.severalIteration(1))
trainSummary.setSummaryTrigger("Parameters", Trigger.severalIteration(20))
```
Notice: "Parameters" show the histogram of parameters and gradParameters in the model. But getting parameters from workers is a heavy operation, recommend setting the trigger interval to at least 10 iterations. For a better visualization, please give names to the layers in model.

* ValidationSummary will show the result of ValidationMethod set in optimizer.setValidation(), like "Loss" and "Top1Accuracy".

* Summary also provide readScalar function to read scalar summary by tag name. Reading "Loss" from summary:
```scala
val trainLoss = trainSummary.readScalar("Loss")
val validationLoss = validationSummary.readScalar("Loss")
```
---
## Known Issues

* Currently, BigDL uses synchronous mini-batch SGD in model training. The mini-batch size is expected to be a multiple of **total cores** used in the job.

* You may observe very poor performance when running BigDL for Spark 2.0 with Java 7; it is highly recommended to use Java 8 when building and running BigDL for Spark 2.0.

* On Spark 2.0, please use default Java serializer instead of Kryo because of [Kryo Issue 341](https://github.com/EsotericSoftware/kryo/issues/341). The issue has been fixed in Kryo 4.0. However, Spark 2.0 uses Kryo 3.0.3. Spark 1.5 and 1.6 do not have this problem.

* On CentOS 6 and 7, please increase the max user processes to a larger value (e.g., 514585); otherwise, you may see errors like "unable to create new native thread".

* Currently, BigDL will load all the training and validation data into memory during training. You may encounter errors if it runs out of memory.

* If you meet the program stuck after **Save model...** on Mesos, check the `spark.driver.memory` and increase the value. Eg, VGG on Cifar10 may need 20G+.

* If you meet `can't find executor core number` on Mesos, you should pass the executor cores through `--conf spark.executor.cores=xxx`