<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>optim.optimizer &#8212; PySpark master documentation</title>
    
    <link rel="stylesheet" href="../../static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'master',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../static/jquery.js"></script>
    <script type="text/javascript" src="../../static/underscore.js"></script>
    <script type="text/javascript" src="../../static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="search" title="Search" href="../../search.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="nav-item nav-item-0"><a href="../../index.html">PySpark master documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for optim.optimizer</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright 2016 The BigDL Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>


<span class="kn">from</span> <span class="nn">util.common</span> <span class="k">import</span> <span class="n">callBigDlFunc</span>
<span class="kn">from</span> <span class="nn">util.common</span> <span class="k">import</span> <span class="n">JavaValue</span>
<span class="kn">from</span> <span class="nn">util.common</span> <span class="k">import</span> <span class="n">callJavaFunc</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">SparkContext</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">distutils.dir_util</span> <span class="k">import</span> <span class="n">mkpath</span>


<span class="kn">import</span> <span class="nn">sys</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">long</span> <span class="o">=</span> <span class="nb">int</span>
    <span class="n">unicode</span> <span class="o">=</span> <span class="nb">str</span>


<div class="viewcode-block" id="MaxIteration"><a class="viewcode-back" href="../../optim.html#optim.optimizer.MaxIteration">[docs]</a><span class="k">class</span> <span class="nc">MaxIteration</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training, </span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.  </span>
<span class="sd">    MaxIteration is a trigger that triggers an action when training reaches </span>
<span class="sd">    the number of iterations specified by &quot;max&quot;. </span>
<span class="sd">    Usually used as end_trigger when creating an Optimizer. </span>

<span class="sd">    &gt;&gt;&gt; maxIteration = MaxIteration(20)</span>
<span class="sd">    creating: createMaxIteration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MaxIteration trigger.</span>

<span class="sd">        :param max: max</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxEpoch"><a class="viewcode-back" href="../../optim.html#optim.optimizer.MaxEpoch">[docs]</a><span class="k">class</span> <span class="nc">MaxEpoch</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    MaxEpoch is a trigger that triggers an action when training reaches</span>
<span class="sd">    the number of epochs specified by &quot;max_epoch&quot;.</span>
<span class="sd">    Usually used as end_trigger when creating an Optimizer.</span>

<span class="sd">    &gt;&gt;&gt; maxEpoch = MaxEpoch(2)</span>
<span class="sd">    creating: createMaxEpoch</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a MaxEpoch trigger.</span>
<span class="sd"> </span>
<span class="sd">        :param max_epoch: max_epoch</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">)</span></div>


<div class="viewcode-block" id="EveryEpoch"><a class="viewcode-back" href="../../optim.html#optim.optimizer.EveryEpoch">[docs]</a><span class="k">class</span> <span class="nc">EveryEpoch</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    EveryEpoch is a trigger that triggers an action when each epoch finishs.</span>
<span class="sd">    Could be used as trigger in setvalidation and setcheckpoint in Optimizer,</span>
<span class="sd">    and also in TrainSummary.set_summary_trigger.</span>

<span class="sd">    &gt;&gt;&gt; everyEpoch = EveryEpoch()</span>
<span class="sd">    creating: createEveryEpoch</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a EveryEpoch trigger.     </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SeveralIteration"><a class="viewcode-back" href="../../optim.html#optim.optimizer.SeveralIteration">[docs]</a><span class="k">class</span> <span class="nc">SeveralIteration</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A trigger specifies a timespot or several timespots during training,</span>
<span class="sd">    and a corresponding action will be taken when the timespot(s) is reached.</span>
<span class="sd">    SeveralIteration is a trigger that triggers an action every &quot;n&quot; </span>
<span class="sd">    iterations.</span>
<span class="sd">    Could be used as trigger in setvalidation and setcheckpoint in Optimizer,</span>
<span class="sd">    and also in TrainSummary.set_summary_trigger.</span>

<span class="sd">    &gt;&gt;&gt; serveralIteration = SeveralIteration(2)</span>
<span class="sd">    creating: createSeveralIteration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a SeveralIteration trigger.</span>

<span class="sd">        :param interval: interval is the &quot;n&quot; where an action is triggered </span>
<span class="sd">           every &quot;n&quot; iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">interval</span><span class="p">)</span></div>


<div class="viewcode-block" id="Poly"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Poly">[docs]</a><span class="k">class</span> <span class="nc">Poly</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate decay policy, where the effective learning rate </span>
<span class="sd">    follows a polynomial decay, to be zero by the max_iteration. </span>
<span class="sd">    Calculation: base_lr (1 - iter/max_iteration) ^ (power)</span>
<span class="sd">   </span>
<span class="sd">    :param power </span>
<span class="sd">    :param max_iteration</span>

<span class="sd">    &gt;&gt;&gt; poly = Poly(0.5, 2)</span>
<span class="sd">    creating: createPoly</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
            <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">power</span><span class="p">,</span> <span class="n">max_iteration</span><span class="p">)</span></div>


<div class="viewcode-block" id="Step"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Step">[docs]</a><span class="k">class</span> <span class="nc">Step</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A learning rate decay policy, where the effective learning rate is </span>
<span class="sd">    calculated as base_lr * gamma ^ (floor(iter / step_size))</span>
<span class="sd">   </span>
<span class="sd">    :param step_size</span>
<span class="sd">    :param gamma</span>

<span class="sd">    &gt;&gt;&gt; step = Step(2, 0.3)</span>
<span class="sd">    creating: createStep</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
            <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div>


<div class="viewcode-block" id="Optimizer"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer">[docs]</a><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An optimizer is in general to minimize any function with respect </span>
<span class="sd">    to a set of parameters. In case of training a neural network, </span>
<span class="sd">    an optimizer tries to minimize the loss of the neural net with </span>
<span class="sd">    respect to its weights/biases, over the training set. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">,</span>
                 <span class="n">training_rdd</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">end_trigger</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">optim_method</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span>
                 <span class="n">state</span><span class="o">=</span><span class="p">{},</span>
                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
       <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Create an optimizer.</span>

<span class="sd">       :param model: the neural net model</span>
<span class="sd">       :param traiing_rdd: the training dataset </span>
<span class="sd">       :param criterion: the loss function</span>
<span class="sd">       :param optim_method: the algorithm to use for optimization, </span>
<span class="sd">          e.g. SGD, Adagrad, etc.</span>
<span class="sd">       :param state: a set of initial configurations for optimizer, </span>
<span class="sd">          provided as a dict e.g. configurable params include: </span>
<span class="sd">          learningRate, learningRateDecay,etc.  </span>
<span class="sd">       :param end_trigger: when to end the optimization</span>
<span class="sd">       :param batch_size: training batch size</span>
<span class="sd">       &quot;&quot;&quot;</span> 
       <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                           <span class="n">training_rdd</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optim_method</span><span class="p">,</span>
                           <span class="n">state</span><span class="p">,</span> <span class="n">end_trigger</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<div class="viewcode-block" id="Optimizer.setvalidation"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer.setvalidation">[docs]</a>    <span class="k">def</span> <span class="nf">setvalidation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">val_rdd</span><span class="p">,</span> <span class="n">trigger</span><span class="p">,</span> <span class="n">val_method</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Top1Accuracy&quot;</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure validation settings. </span>

<span class="sd">        :param batch_size: validation batch size</span>
<span class="sd">        :param val_rdd: validation dataset</span>
<span class="sd">        :param trigger: validation interval</span>
<span class="sd">        :param val_method: the ValidationMethod to use,</span>
<span class="sd">           e.g. &quot;Top1Accuracy&quot;, &quot;Top5Accuracy&quot;, &quot;Loss&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setValidation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                      <span class="n">trigger</span><span class="p">,</span> <span class="n">val_rdd</span><span class="p">,</span> <span class="n">val_method</span><span class="p">)</span></div>

<div class="viewcode-block" id="Optimizer.setcheckpoint"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer.setcheckpoint">[docs]</a>    <span class="k">def</span> <span class="nf">setcheckpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_trigger</span><span class="p">,</span>
                      <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">isOverWrite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure checkpoint settings. </span>

<span class="sd">        :param checkpoint_trigger: the interval to write snapshots</span>
<span class="sd">        :param checkpoint_path: the path to write snapshots into</span>
<span class="sd">        :param isOverWrite: whether to overwrite existing snapshots in path. </span>
<span class="sd">           default is True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>
            <span class="n">mkpath</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setCheckPoint&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">checkpoint_trigger</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">isOverWrite</span><span class="p">)</span></div>

    <span class="c1"># return a module</span>
<div class="viewcode-block" id="Optimizer.optimize"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Do an optimization. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jmodel</span> <span class="o">=</span> <span class="n">callJavaFunc</span><span class="p">(</span><span class="n">SparkContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">optimize</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">nn.layer</span> <span class="k">import</span> <span class="n">Model</span>
        <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">jmodel</span><span class="p">)</span></div>

<div class="viewcode-block" id="Optimizer.set_train_summary"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer.set_train_summary">[docs]</a>    <span class="k">def</span> <span class="nf">set_train_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set train summary. A TrainSummary object contains information </span>
<span class="sd">        necesary for the optimizer to know how often the logs are recorded, </span>
<span class="sd">        where to store the logs and how to retrieve them, etc. For details, </span>
<span class="sd">        refer to the docs of TrainSummary.</span>

<span class="sd">        :param summary: a TrainSummary object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setTrainSummary&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">summary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Optimizer.set_val_summary"><a class="viewcode-back" href="../../optim.html#optim.optimizer.Optimizer.set_val_summary">[docs]</a>    <span class="k">def</span> <span class="nf">set_val_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set validation summary. A ValidationSummary object contains information </span>
<span class="sd">        necesary for the optimizer to know how often the logs are recorded, </span>
<span class="sd">        where to store the logs and how to retrieve them, etc. For details, </span>
<span class="sd">        refer to the docs of ValidationSummary.</span>

<span class="sd">        :param summary: a ValidationSummary object</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;setValSummary&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">summary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="TrainSummary"><a class="viewcode-back" href="../../optim.html#optim.optimizer.TrainSummary">[docs]</a><span class="k">class</span> <span class="nc">TrainSummary</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">,</span> <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A logging facility which allows user to trace how indicators (e.g. </span>
<span class="sd">    learning rate, training loss, throughput, etc.) change with iterations/time </span>
<span class="sd">    in an optimization process. TrainSummary is for training indicators only </span>
<span class="sd">    (check ValidationSummary for validation indicators).  It contains necessary </span>
<span class="sd">    information for the optimizer to know where to store the logs, how to </span>
<span class="sd">    retrieve the logs, and so on. - The logs are written in tensorflow-compatible </span>
<span class="sd">    format so that they can be visualized directly using tensorboard. Also the </span>
<span class="sd">    logs can be retrieved as ndarrays and visualized using python libraries </span>
<span class="sd">    such as matplotlib (in notebook, etc.). </span>
<span class="sd">    </span>
<span class="sd">    Use optimizer.setTrainSummary to enable train logger. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a TrainSummary. Logs will be saved to log_dir/app_name/train.</span>

<span class="sd">        :param log_dir: the root dir to store the logs</span>
<span class="sd">        :param app_name: the application name</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">)</span>

<div class="viewcode-block" id="TrainSummary.read_scalar"><a class="viewcode-back" href="../../optim.html#optim.optimizer.TrainSummary.read_scalar">[docs]</a>    <span class="k">def</span> <span class="nf">read_scalar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tag</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve train logs by type. Return an array of records in the format </span>
<span class="sd">        (step,value,wallClockTime). - &quot;Step&quot; is the iteration count by default.</span>
<span class="sd">        </span>
<span class="sd">        :param tag: the type of the logs, Supported tags are: &quot;LearningRate&quot;, </span>
<span class="sd">           &quot;Loss&quot;, &quot;Throughput&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summaryReadScalar&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="TrainSummary.set_summary_trigger"><a class="viewcode-back" href="../../optim.html#optim.optimizer.TrainSummary.set_summary_trigger">[docs]</a>    <span class="k">def</span> <span class="nf">set_summary_trigger</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trigger</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the interval of recording for each indicator. </span>
<span class="sd">        </span>
<span class="sd">        :param tag: tag name. Supported tag names are &quot;LearningRate&quot;, &quot;Loss&quot;, </span>
<span class="sd">           &quot;Throughput&quot;, &quot;Parameters&quot;. &quot;Parameters&quot; is an umbrella tag that </span>
<span class="sd">           includes weight, bias, gradWeight, gradBias, and some running status</span>
<span class="sd">           (eg. runningMean and runningVar in BatchNormalization). If you </span>
<span class="sd">           didn&#39;t set any triggers, we will by default record LearningRate, </span>
<span class="sd">           Loss and Throughput in each iteration, while *NOT* recording </span>
<span class="sd">           Parameters, as recording parameters may introduce substantial </span>
<span class="sd">           overhead when the model is very big.</span>
<span class="sd">        :param trigger: trigger</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summarySetTrigger&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">name</span><span class="p">,</span> <span class="n">trigger</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ValidationSummary"><a class="viewcode-back" href="../../optim.html#optim.optimizer.ValidationSummary">[docs]</a><span class="k">class</span> <span class="nc">ValidationSummary</span><span class="p">(</span><span class="n">JavaValue</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     A logging facility which allows user to trace how indicators (e.g. </span>
<span class="sd">     validation loss, top1 accuray, top5 accuracy etc.) change with </span>
<span class="sd">     iterations/time in an optimization process. ValidationSummary is for </span>
<span class="sd">     validation indicators only (check TrainSummary for train indicators).  </span>
<span class="sd">     It contains necessary information for the optimizer to know where to </span>
<span class="sd">     store the logs, how to retrieve the logs, and so on. - The logs are </span>
<span class="sd">     written in tensorflow-compatible format so that they can be visualized </span>
<span class="sd">     directly using tensorboard. Also the logs can be retrieved as ndarrays </span>
<span class="sd">     and visualized using python libraries such as matplotlib </span>
<span class="sd">     (in notebook, etc.).</span>
<span class="sd">    </span>
<span class="sd">     Use optimizer.setValidationSummary to enable validation logger.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a ValidationSummary. Logs will be saved to </span>
<span class="sd">        log_dir/app_name/train. By default, all ValidationMethod set into </span>
<span class="sd">        optimizer will be recorded and the recording interval is the same </span>
<span class="sd">        as trigger of ValidationMethod in the optimizer. </span>

<span class="sd">        :param log_dir: the root dir to store the logs</span>
<span class="sd">        :param app_name: the application name</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">JavaValue</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">app_name</span><span class="p">)</span>

<div class="viewcode-block" id="ValidationSummary.read_scalar"><a class="viewcode-back" href="../../optim.html#optim.optimizer.ValidationSummary.read_scalar">[docs]</a>    <span class="k">def</span> <span class="nf">read_scalar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tag</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve validation logs by type. Return an array of records in the </span>
<span class="sd">        format (step,value,wallClockTime). - &quot;Step&quot; is the iteration count </span>
<span class="sd">        by default.</span>

<span class="sd">        :param tag: the type of the logs. The tag should match the name of </span>
<span class="sd">           the ValidationMethod set into the optimizer. e.g. </span>
<span class="sd">           &quot;Top1AccuracyLoss&quot;,&quot;Top1Accuracy&quot; or &quot;Top5Accuracy&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callBigDlFunc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bigdl_type</span><span class="p">,</span> <span class="s2">&quot;summaryReadScalar&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                             <span class="n">tag</span><span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">SparkContext</span>
    <span class="kn">from</span> <span class="nn">optim</span> <span class="k">import</span> <span class="n">optimizer</span>
    <span class="kn">from</span> <span class="nn">util.common</span> <span class="k">import</span> <span class="n">init_engine</span>
    <span class="kn">from</span> <span class="nn">util.common</span> <span class="k">import</span> <span class="n">create_spark_conf</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s2">&quot;local[4]&quot;</span><span class="p">,</span> <span class="n">appName</span><span class="o">=</span><span class="s2">&quot;test optimizer&quot;</span><span class="p">,</span>
                      <span class="n">conf</span><span class="o">=</span><span class="n">create_spark_conf</span><span class="p">())</span>
    <span class="n">init_engine</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span><span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
                                                  <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="nav-item nav-item-0"><a href="../../index.html">PySpark master documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.5.
    </div>
  </body>
</html>